{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4826f6fe",
   "metadata": {},
   "source": [
    "# Regression Models Analysis on House Sales Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b77c5f6",
   "metadata": {},
   "source": [
    "# Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c536b8",
   "metadata": {},
   "source": [
    "ABC company intereste to buy property in King County. They are new to the area, and interest about the housing in Seattle. They want to decide if they should buy property in Seattle or other area within King County.\n",
    "\n",
    "The goal is to use regression models to predict sale price and translate these findings into actionable insights for ABC company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e7b8d9",
   "metadata": {},
   "source": [
    "Library that are used for the data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b1423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# scaling and train test split\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#one hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#evalutate multicollinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# for QQ plot\n",
    "import scipy.stats as stats\n",
    "# evaluation on test data\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334baf11",
   "metadata": {},
   "source": [
    "King County House Sales dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa010ee2",
   "metadata": {},
   "source": [
    "\n",
    "This data set can be found in kc_house_data.csv in the following GitHub repository\n",
    "https://github.com/learn-co-curriculum/dsc-phase-2-project-v2-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use this table to do regression analysis to analyze King county house sales and predict house sale price\n",
    "file_path = \"\\\\Users\\\\eggfr\\\\Flatiron\\\\Flatiron_phase2_project\\\\dsc-phase-2-project\\\\data\\\\kc_house_data.csv\"\n",
    "project2_raw_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad0e704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(project2_raw_df.corr(),annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa3b55",
   "metadata": {},
   "source": [
    "Column Names and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3dd5f2",
   "metadata": {},
   "source": [
    "id: Unique ID for each home sold\n",
    "date: Date of the home sale\n",
    "price: Price of each home sold\n",
    "bedrooms: Number of bedrooms\n",
    "bathrooms: Number of bathrooms, where .5 accounts for a room with a toilet but no shower\n",
    "sqft_living: Square footage of the apartments interior living space\n",
    "sqft_lot: Square footage of the land space\n",
    "floors: Number of floors\n",
    "waterfront: - A dummy variable for whether the apartment was overlooking the waterfront or not\n",
    "view: An index from 0 to 4 of how good the view of the property was\n",
    "condition: - An index from 1 to 5 on the condition of the apartment,\n",
    "grade: An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\n",
    "sqft_above: The square footage of the interior housing space that is above ground level\n",
    "sqft_basement: The square footage of the interior housing space that is below ground level\n",
    "yr_built: The year the house was initially built\n",
    "yr_renovated: The year of the houseâ€™s last renovation\n",
    "zipcode: What zipcode area the house is in\n",
    "lat: Lattitude\n",
    "long: Longitude\n",
    "sqft_living15: The square footage of interior housing living space for the nearest 15 neighbors\n",
    "sqft_lot15: The square footage of the land lots of the nearest 15 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35becb4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "project2_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea890e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = project2_raw_df['price']\n",
    "X= project2_raw_df.drop('price',1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f42778",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d604deea",
   "metadata": {},
   "source": [
    "Separating data into training and testing sets is an important part of evaluating the models.Most of the data is used for training, and a smaller portion of the data is used for testing. For this analysis: we only split data into train and test. 75% of the data is for training and 25% for test. Also, the data split happened before we even do any EDA analysis to prevent data leakage. There is 16197 row of datas for the train set and 5400 rows of the data for test set before any data cleaning or analysis is done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train-test set using 75%-25% ratio for the train set and test set and set the random state = 42) randomly split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y ,test_size=0.25,random_state=42)\n",
    "# shape of train and test splits\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4c165",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22280443",
   "metadata": {},
   "source": [
    "# Checking category type for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b58d2b",
   "metadata": {},
   "source": [
    "OLS analysis only take data type in numerical form (int64 or float64). We need to change the categotical data into numerical form. Also, we need to check missing data for each column. The section below shows there is missing values on the category of waterfront, view, and year renovated. We will go through every category and clean the data as necessary before OLS analysis.\n",
    "\n",
    "We need to check missing value and data type. We need to identify categorical nominal variable and categotical oridnal variable. For categorical nominal variable, we need to transform it to numerical form using One Hot Encdoing method. No adjustment is needed for categotical ordinal variable.\n",
    "\n",
    "Also, We have to make changes for both sets since we split our data into train set and test set. Otherwise, the analysis will be incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6a634",
   "metadata": {},
   "source": [
    "Checking datatype of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a0a4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "project2_raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8f378",
   "metadata": {},
   "source": [
    "Checking missing value of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c61de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine count of missing values in table\n",
    "project2_raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a2d18b",
   "metadata": {},
   "source": [
    "Preprocessing date-month-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8ca56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Split date, year, month into seperate columns\n",
    "x_train['date'] = pd.to_datetime(x_train['date'])\n",
    "x_train['month'] = x_train['date'].apply(lambda date:date.month)\n",
    "x_train['year'] = x_train['date'].apply(lambda date:date.year)\n",
    "x_train = x_train.drop('date',axis=1)\n",
    "\n",
    "x_test['date'] = pd.to_datetime(x_test['date'])\n",
    "x_test['month'] = x_test['date'].apply(lambda date:date.month)\n",
    "x_test['year'] = x_test['date'].apply(lambda date:date.year)\n",
    "x_test = x_test.drop('date',axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38cbccc",
   "metadata": {},
   "source": [
    "Preprocessing sqft_basement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d3324",
   "metadata": {},
   "source": [
    "There is missing value in \"sqft_basement\" variable. I will replace the 1 missing value with the mean of the value, which is 297."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a502da9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking unique category for column ('sqft_basement'), there is a datadype'?''.we replace it withe mean of value.\n",
    "project2_raw_df['sqft_basement'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab99b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the inproper datatype for 'sqft_basement' and replace it withe mean of value, 297.\n",
    "x_train['sqft_basement'] = x_train['sqft_basement'].apply(lambda x: float(x.replace(\"?\", \"297\")))\n",
    "x_train['sqft_basement'] = x_train['sqft_basement'].apply(lambda x: float(x))\n",
    "#x_train['sqft_basement'].dtype\n",
    "x_test['sqft_basement'] = x_test['sqft_basement'].apply(lambda x: float(x.replace(\"?\", \"297\")))\n",
    "x_test['sqft_basement'] = x_test['sqft_basement'].apply(lambda x: float(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2f60f",
   "metadata": {},
   "source": [
    "Preprocessing floors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b2181",
   "metadata": {},
   "source": [
    "Checking the category type for variable \"floors\". No action is needed as it is categorical ordinal variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6e978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the category of floors- categorical ordinal variable. No adjustment is needed.\n",
    "project2_raw_df['floors'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecabe03",
   "metadata": {},
   "source": [
    "Preprocessing waterfront"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc73ff",
   "metadata": {},
   "source": [
    "There is missing value in \"waterfront\" variable, and majority of the \"waterfront\" is None. I will replace the missing values with No data type. Also, waterfront is also a categorical variable, it will be transformed into 1 for yes and 0 for no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b1438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# percentage of waterfront in the sale data.\n",
    "project2_raw_df['waterfront'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684bd8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill na with no value and change yes to 1 n no to 0 #since majority of the sales has no waterfront.\n",
    "x_train['waterfront'] = x_train['waterfront'].fillna(value = 'NO')\n",
    "x_train['waterfront'] = x_train['waterfront'].replace(to_replace = ['YES','NO'],value = [1,0])\n",
    "x_train['waterfront'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb03593",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test['waterfront'] = x_test['waterfront'].fillna(value = 'NO')\n",
    "x_test['waterfront'] = x_test['waterfront'].replace(to_replace = ['YES','NO'],value = [1,0])\n",
    "x_test['waterfront'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d306e6c",
   "metadata": {},
   "source": [
    "Preprocessing view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699959d9",
   "metadata": {},
   "source": [
    "There is missing value in \"view\" variable, and majority of the \"view\" is None. I will replace the missing values with None data type. Also, view is also a categorical variable, it will be transformed into 0 to 4 with 0 as none and 4 as excellent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cda7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "project2_raw_df['view'].value_counts(normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093546dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na with no value -->majority is none and could be N/A as none as well. n change the catgorical ordinal in to 0 to 4 with 0 to none and 4 to excellent\n",
    "x_train['view'] = x_train['view'].fillna(value = 'NONE')\n",
    "x_test['view'] = x_test['view'].fillna(value = 'NONE')\n",
    "x_train['view'] = x_train['view'].replace(to_replace = ['NONE','AVERAGE','GOOD','FAIR','EXCELLENT'],value = [0,1,2,3,4])\n",
    "x_test['view'] = x_test['view'].replace(to_replace = ['NONE','AVERAGE','GOOD','FAIR','EXCELLENT'],value = [0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16396a8f",
   "metadata": {},
   "source": [
    "Preprocessing Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81203850",
   "metadata": {},
   "source": [
    "There is no missing value on 'condition' variable, so no replacement is needed. 'Condition' is a categorical ordinal variable, it will be transformed into 0 to 4 with 0 as poor and 4 as very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02149591",
   "metadata": {},
   "outputs": [],
   "source": [
    "project2_raw_df['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06655ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace cateogrical rating with (0 to 4 scale. 0 - Poor, and 4 - Very Good)\n",
    "x_train['condition'] = x_train['condition'].replace(to_replace = ['Poor','Fair','Average','Good','Very Good'],value = [0,1,2,3,4])\n",
    "x_test['condition'] = x_test['condition'].replace(to_replace = ['Poor','Fair','Average','Good','Very Good'],value = [0,1,2,3,4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc2918",
   "metadata": {},
   "source": [
    "Preprocessing Grade and create a new_grade column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a741f734",
   "metadata": {},
   "source": [
    "There is no missing value on 'grade' variable, so no replacement is needed. However, a \"new_grade\" column is created and stored all the numerical value for the grade. As given from the data set, graded are scaled from 3 to 13, poor to mension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4262ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "project2_raw_df['grade'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfe96a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# grab the numerical rating and assign it to interger type\n",
    "x_train['new_grade'] = x_train['grade'].astype(str).str[0]\n",
    "x_train['new_grade'] = x_train['new_grade'].astype(int)\n",
    "x_test['new_grade'] = x_test['grade'].astype(str).str[0]\n",
    "x_test['new_grade'] = x_test['new_grade'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the string grade column\n",
    "x_train = x_train.drop(columns='grade')\n",
    "x_test = x_test.drop(columns='grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c2815",
   "metadata": {},
   "source": [
    "Preprocessing bedrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620735bc",
   "metadata": {},
   "source": [
    "Checking the category type for variable \"bedrooms\". No action is needed as it is categorical ordinal variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "project2_raw_df['bedrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bceeee",
   "metadata": {},
   "source": [
    "Preprocessing bathrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2eebe",
   "metadata": {},
   "source": [
    "Checking the category type for variable \"bathrooms\". No action is needed as it is categorical ordinal variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1fc469",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "project2_raw_df['bathrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e3255",
   "metadata": {},
   "source": [
    "Set up a new group \"seattle\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d2066",
   "metadata": {},
   "source": [
    "Going to create a new column as seattle by zipcode which it solely belongs https://www.ciclt.net/sn/clt/capitolimpact/gw_ziplist.aspx?FIPS=53033 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping seattle into zipcode which it solely belongs https://www.ciclt.net/sn/clt/capitolimpact/gw_ziplist.aspx?FIPS=53033 \n",
    "x_train['seattle'] = x_train['zipcode'].apply(lambda x : 0 if x == 98101 or x == 98102 or x == 98103 or x == 98104 or x == 98105 or x == 98106 or x == 98107 or x == 98109 or x == 98111 or x == 98112 or x == 98114 or x == 98116 or x == 98117 or x == 98118 or x == 98119 or x == 98121 or x == 98122 or x == 98124 or x == 98125 or x == 98126 or x == 98131 or x == 98132 or x == 98133 or x == 98134 or x == 98136 or x == 98144 or x == 98145 or x == 98146 or x == 98148 or x == 98154  or x == 98160 or x == 98161 or x == 98164 or x == 98166 or x == 98171 or x == 98174 or x== 98178 or x == 98199 else 1) \n",
    "x_test['seattle'] = x_test['zipcode'].apply(lambda x : 0 if x == 98101 or x == 98102 or x == 98103 or x == 98104 or x == 98105 or x == 98106 or x == 98107 or x == 98109 or x == 98111 or x == 98112 or x == 98114 or x == 98116 or x == 98117 or x == 98118 or x == 98119 or x == 98121 or x == 98122 or x == 98124 or x == 98125 or x == 98126 or x == 98131 or x == 98132 or x == 98133 or x == 98134 or x == 98136 or x == 98144 or x == 98145 or x == 98146 or x == 98148 or x == 98154  or x == 98160 or x == 98161 or x == 98164 or x == 98166 or x == 98171 or x == 98174 or x== 98178 or x == 98199 else 1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae532a",
   "metadata": {},
   "source": [
    "Preprocessing zipcode via One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000fd60",
   "metadata": {},
   "source": [
    "While zipcode is numerical variable in the data table, it is categorical nominal variable. Different zipcode means different category. We are going to transform it into a numerical variable via One Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c40278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#onehot Econdinf zipcode for x_train\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "data =  x_train[['zipcode']]\n",
    "\n",
    "view_df = pd.DataFrame(ohe.fit_transform(data).toarray(), index = x_train.index) #index = x_train.index to match x_train index for concat later\n",
    "view_df.columns = ohe.get_feature_names()   #use get_feature_names() to get feature name back after one hot encoding\n",
    "\n",
    "x_train = pd.concat([x_train,view_df],axis=1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3effd969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot Econdinf zipcode for x_test\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "data =  x_test[['zipcode']]\n",
    "\n",
    "view_df = pd.DataFrame(ohe.fit_transform(data).toarray(), index = x_test.index) #index = x_train.index to match x_train index for concat later\n",
    "view_df.columns = ohe.get_feature_names()   #use get_feature_names() to get feature name back after one hot encoding\n",
    "\n",
    "x_test = pd.concat([x_test,view_df],axis=1)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f2cbb",
   "metadata": {},
   "source": [
    "Preprocessing yr_renovated and create is_renovated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3d8b8",
   "metadata": {},
   "source": [
    "yr_renovated is an indication if the house is renovated. There is 0.0 in the value counts and that shows there is a category that house that is not renovated. With majority of the sale is not renovated. I would assume the missing value is just not renovated, and filled the missing value with 0. Also, I also change this to a categoty variable with 0 = not renovated and 1 is renovated and stored it in a var variable, is_renovated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f723ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "project2_raw_df['yr_renovated'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82860266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna with 0 to NaN for year_renovated- assuming there is no renovation.\n",
    "x_train['yr_renovated'] = x_train['yr_renovated'].fillna(value = 0)\n",
    "x_test['yr_renovated'] = x_test['yr_renovated'].fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93530891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train['is_renovated'] = x_train['yr_renovated'].apply(lambda x: 0 if x ==0 else 1)\n",
    "x_test['is_renovated'] = x_test['yr_renovated'].apply(lambda x: 0 if x ==0 else 1)\n",
    "#x_test['sqft_basement'] = x_test['sqft_basement'].apply(lambda x: float(x.replace(\"?\", \"297\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c0eeb",
   "metadata": {},
   "source": [
    "# Log normal the price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40b9a3",
   "metadata": {},
   "source": [
    "Price look positively skewed with a right tail, and we may have to account for outliers later on of the analysis. For now, lets log the price so we have amore normalized distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa5dd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = sns.kdeplot(y_train)\n",
    "p.set( title = \"Distribution of Home Sale Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a log on price\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ba70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.kdeplot(y_train)\n",
    "p.set( title = \"Distribution of Home Sale Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfbbf1",
   "metadata": {},
   "source": [
    "Recheck our x_train data type and missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7b510",
   "metadata": {},
   "source": [
    "Let's recheck our data type to see if everything is numerical form and also check if we have any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14c50e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c326fe1",
   "metadata": {},
   "source": [
    "Every category is non-null and is in either int64 or float64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5223acbf",
   "metadata": {},
   "source": [
    "scale data w MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b248cc5",
   "metadata": {},
   "source": [
    "Since 'sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15' are measured in different units, they will be scaled with MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be795f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data w MinMaxScaler 'sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15 for both train n data set\n",
    "features = ['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15']\n",
    "autoscaler = MinMaxScaler()\n",
    "\n",
    "x_train[features] = autoscaler.fit_transform(x_train[features])\n",
    "x_test[features] = autoscaler.fit_transform(x_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4311a2",
   "metadata": {},
   "source": [
    "Dropping some columns that are not useful for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43858f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.drop(columns=['id','year','zipcode'])\n",
    "x_test = x_test.drop(columns=['id','year','zipcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c843ee1",
   "metadata": {},
   "source": [
    "# Baseline Model (with every predictors from the data set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b6ba8f",
   "metadata": {},
   "source": [
    "I decided to use every predictors from the data as my baseline model and just to get a feeling on how this would work with linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240af33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "y = y_train\n",
    "X= x_train\n",
    "z = x_test\n",
    "Hh = y_test\n",
    "Xcont = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(endog = y, exog = Xcont)\n",
    "model = sm.OLS(y,Xcont)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0508fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = LinearRegression()\n",
    "\n",
    "testsmodel = ols.fit(X,y)\n",
    "print(testsmodel.score(X,y)) # train\n",
    "print(testsmodel.score(z,Hh)) # test \n",
    "#print(cross_val_score(lr,X,y,cv=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ddc567",
   "metadata": {},
   "source": [
    "Our baseline R2 is at 0.87. However, I am using every predictors that could be found in the dataset to achieve this. Obviously, most of our variance can be explained with every predictors as inputs.\n",
    "\n",
    "We'll use this as a baseline moving forward and see if we can keep our R2 score close to 0.87\n",
    "\n",
    "The Training and Test scores are with 5%, so it shows me that the our model is not underfit/overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_predictions = ols.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e27a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.scatter(np.exp(model1_predictions), np.exp(y_test))\n",
    "plt.xlabel('predicting testing price in $')\n",
    "plt.ylabel('actual testing sale price in $')\n",
    "fig.suptitle('prediction price vs actual sale price')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebaa7ff",
   "metadata": {},
   "source": [
    "Check the Normality Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcba85d3",
   "metadata": {},
   "source": [
    "Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line thatâ€™s roughly straight.. We are plotting our model residual and normal data. There is a tail on the left hand side, and we have to eliminate that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131b540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(res.resid, dist=stats.norm, line='45', fit=True)\n",
    "ax.set_title(\"QQ-plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b57d29",
   "metadata": {},
   "source": [
    "Baseline model residual plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a1382",
   "metadata": {},
   "source": [
    "Clear underestimate when the sales price increase.Further subsetting is potentially warranted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4709b62f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(model1_predictions,(y_test-model1_predictions))\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.title('baseline residual plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d4ef17",
   "metadata": {},
   "source": [
    "# 2nd Model. Feature enginering with the investigation of Multicollinearity via VIF test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71d949",
   "metadata": {},
   "source": [
    "All predictors are used in the baselinVe model.There are some chances that the baseline has strong multicollinearity issues. Varince_inflation_factor is used to feature select some predictors.\n",
    "When WIF score is higher than 5, we consider it has some collinearity issue with other predicors. In this model, we will remove features that has VIF score higher than 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform vif_test\n",
    "x_train_list= list(x_train.columns.values)\n",
    "feature = x_train_list\n",
    "X = x_train[feature]\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif_list = list(zip(feature, vif))\n",
    "vif_scores = list(zip(feature, vif))\n",
    "x_cols = [x for x,vif in vif_scores if vif < 5]\n",
    "print(len(vif_scores), len(x_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd4865",
   "metadata": {},
   "source": [
    "We reduce our predictors from 89 to 35 after the VIF test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f53eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_train\n",
    "Hh = y_test\n",
    "feature = x_cols\n",
    "#feature = ['bedrooms', 'bathrooms', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15', 'month', 'new_grade', 'x0_98002', 'x0_98003', 'x0_98005', 'x0_98007', 'x0_98010', 'x0_98022', 'x0_98023', 'x0_98024', 'x0_98030', 'x0_98031', 'x0_98032', 'x0_98039', 'x0_98040', 'x0_98042', 'x0_98055', 'x0_98056', 'x0_98058', 'x0_98070', 'x0_98092', 'x0_98108', 'x0_98168', 'x0_98188', 'x0_98198']\n",
    "#feature = ['bedrooms', 'bathrooms', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15', 'month','new_grade']\n",
    "\n",
    "X= x_train[feature]\n",
    "z= x_test[feature]\n",
    "Xcont = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(endog = y, exog = Xcont)\n",
    "model = sm.OLS(y,Xcont)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcbe24f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ols2 = LinearRegression()\n",
    "testsmodel = ols2.fit(X,y)\n",
    "print(testsmodel.score(X,y)) # train\n",
    "print(testsmodel.score(z,Hh)) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_predictions = ols2.predict(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06bc21a",
   "metadata": {},
   "source": [
    "Model2 R2 is at 0.69. However, and some predictors is eliminated while R2 score decreases. \n",
    "\n",
    "The Training and Test scores are with 3%, so it shows me that the our model is not underfit/overfit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8d04a",
   "metadata": {},
   "source": [
    "There is outliers that should be eliminated and see how that affect the model if outliers is eliminated in later models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695a062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.scatter(np.exp(model2_predictions), np.exp(y_test))\n",
    "plt.xlabel('sale price in $')\n",
    "plt.xlabel('predicting testing price in $')\n",
    "plt.ylabel('actual testing sale price in $')\n",
    "fig.suptitle('prediction price vs actual sale price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359a7de",
   "metadata": {},
   "source": [
    "QQ-plot : points forming a line thatâ€™s roughly straight line- left tail is reducing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf314fcd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(res.resid, dist=stats.norm, line='45', fit=True)\n",
    "ax.set_title(\"QQ-plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257db011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.exp(model2_predictions),(np.exp(y_test)-np.exp(model2_predictions)))\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.title(\"model2 residual plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682cbef",
   "metadata": {},
   "source": [
    "# 3rd Model. Feature enginering with eliminating outliers from model 2 and subsetting some inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f1cda",
   "metadata": {},
   "source": [
    "As working from the 2nd model- I see some outliers from the sale price- Lets take a look on the historgram from the original data. I am going to use 97% percentile as our subsetting range for price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e36d1",
   "metadata": {},
   "source": [
    "project2_raw_df.price.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb40f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(90, 99):\n",
    "    q = i / 100\n",
    "    print('{} percentile: {}'.format(q, project2_raw_df['price'].quantile(q=q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f0128",
   "metadata": {},
   "source": [
    "Also, I am looking at the highest coefficient from the 2nd model, and reduce outliers from the ones that have high coefficeient. The categories that will be removed some outliers as we follow.\n",
    "sqft_lot is using 0.95 percentile, which is 43307\n",
    "sqft_living is using 0.96 percentile, which is 3920\n",
    "Bedrooms category is subsetting to 12 to remove the 33 bedrooom outlier\n",
    "Bathrooms category is subsetting to 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6040413",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(90, 99):\n",
    "    q = i / 100\n",
    "    print('{} percentile: {}'.format(q, project2_raw_df['sqft_lot'].quantile(q=q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(90, 99):\n",
    "    q = i / 100\n",
    "    print('{} percentile: {}'.format(q, project2_raw_df['sqft_living'].quantile(q=q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18cfd8",
   "metadata": {},
   "source": [
    "As we there is 33 bedrooms in our dataset, I would consider that as an outlier and drop it from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb094ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "project2_raw_df['bedrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f633cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "project2_raw_df['bathrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_project2_raw_df =project2_raw_df\n",
    "rem_project2_raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b576c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rem_project2_raw_df = rem_project2_raw_df.loc[rem_project2_raw_df['price']< 11600000]\n",
    "rem_project2_raw_df = rem_project2_raw_df.loc[rem_project2_raw_df['bathrooms']< 6]\n",
    "rem_project2_raw_df = rem_project2_raw_df.loc[rem_project2_raw_df['bedrooms']< 12]\n",
    "rem_project2_raw_df = rem_project2_raw_df.loc[rem_project2_raw_df['sqft_living']< 3920]\n",
    "rem_project2_raw_df = rem_project2_raw_df.loc[rem_project2_raw_df['sqft_lot']< 43307]\n",
    "rem_project2_raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57a13e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rem_project2_raw_df.price.hist()\n",
    "plt.title('price histogram')\n",
    "plt.xlabel('sale price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94fa48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rem_project2_raw_df.bedrooms.hist()\n",
    "plt.title('bedroom histogram')\n",
    "plt.xlabel('number of bedroom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10220a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rem_project2_raw_df.sqft_living.hist()\n",
    "plt.title('sqft_living histogram')\n",
    "plt.xlabel('sqft_living')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dab577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rem_project2_raw_df.sqft_lot.hist()\n",
    "plt.title('sqft_lot histogram')\n",
    "plt.xlabel('sqft_lot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f367c7",
   "metadata": {},
   "source": [
    "Re-create the data set for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfef668",
   "metadata": {},
   "source": [
    "Since I add the conditions for several predictors, the size of the original x_train,x_test data set is going to be different. Hence, I need to recreate the x_train as x_train1, x_test as x_test1, y_train as y_train1, and y_test1. Also, two more cities categories are created in this so \"seattle\" data can be compated. The data set is dropped from 21597 to 19880."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a069b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "rem_y = rem_project2_raw_df['price']\n",
    "rem_X= rem_project2_raw_df.drop('price',1)\n",
    "rem_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e73de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_y = rem_project2_raw_df['price']\n",
    "rem_X= rem_project2_raw_df.drop('price',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train-test set using 75-25 (train-test and random state = 42) randomly split the data\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(rem_X, rem_y ,test_size=0.25,random_state=42)\n",
    "# shape of train and test splits\n",
    "x_train1.shape, x_test1.shape, y_train1.shape, y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_project2_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5591e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_project2_raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_project2_raw_df['seattle'] = rem_project2_raw_df['zipcode'].apply(lambda x : 0 if x == 98101 or x == 98102 or x == 98103 or x == 98104 or x == 98105 or x == 98106 or x == 98107 or x == 98109 or x == 98111 or x == 98112 or x == 98114 or x == 98116 or x == 98117 or x == 98118 or x == 98119 or x == 98121 or x == 98122 or x == 98124 or x == 98125 or x == 98126 or x == 98131 or x == 98132 or x == 98133 or x == 98134 or x == 98136 or x == 98144 or x == 98145 or x == 98146 or x == 98148 or x == 98154  or x == 98160 or x == 98161 or x == 98164 or x == 98166 or x == 98171 or x == 98174 or x== 98178 or x == 98199 else 1) \n",
    "rem_project2_raw_df['seattle'] = rem_project2_raw_df['zipcode'].apply(lambda x : 0 if x == 98101 or x == 98102 or x == 98103 or x == 98104 or x == 98105 or x == 98106 or x == 98107 or x == 98109 or x == 98111 or x == 98112 or x == 98114 or x == 98116 or x == 98117 or x == 98118 or x == 98119 or x == 98121 or x == 98122 or x == 98124 or x == 98125 or x == 98126 or x == 98131 or x == 98132 or x == 98133 or x == 98134 or x == 98136 or x == 98144 or x == 98145 or x == 98146 or x == 98148 or x == 98154  or x == 98160 or x == 98161 or x == 98164 or x == 98166 or x == 98171 or x == 98174 or x== 98178 or x == 98199 else 1) \n",
    "rem_project2_raw_df['kent'] = rem_project2_raw_df['zipcode'].apply(lambda x : 0 if x == 98030 or x == 98031 or x == 98032 or x == 98035 or x == 98042 or x == 98064 else 1)\n",
    "rem_project2_raw_df['kent'] = rem_project2_raw_df['zipcode'].apply(lambda x : 0 if x == 98030 or x == 98031 or x == 98032 or x == 98035 or x == 98042 or x == 98064 else 1)\n",
    "rem_project2_raw_df['bellevue'] = rem_project2_raw_df['zipcode'].apply(lambda x : 0 if x == 98004 or x == 98005 or x == 98006 or x == 98007 or x == 98008 or x == 98009 or x == 98015 else 1)\n",
    "rem_project2_raw_df['bellevue'] = rem_project2_raw_df['zipcode'].apply(lambda x : 0 if x == 98004 or x == 98005 or x == 98006 or x == 98007 or x == 98008 or x == 98009 or x == 98015 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725768ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_project2_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01047f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1['date'] = pd.to_datetime(x_train1['date'])\n",
    "x_train1['month'] = x_train1['date'].apply(lambda date:date.month)\n",
    "x_train1['year'] = x_train1['date'].apply(lambda date:date.year)\n",
    "x_train1 = x_train1.drop('date',axis=1)\n",
    "\n",
    "x_test1['date'] = pd.to_datetime(x_test1['date'])\n",
    "x_test1['month'] = x_test1['date'].apply(lambda date:date.month)\n",
    "x_test1['year'] = x_test1['date'].apply(lambda date:date.year)\n",
    "x_test1 = x_test1.drop('date',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887cc57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the inproper datatype for 'sqft_basement'\n",
    "x_train1['sqft_basement'] = x_train1['sqft_basement'].apply(lambda x: float(x.replace(\"?\", \"297\")))\n",
    "x_train1['sqft_basement'] = x_train1['sqft_basement'].apply(lambda x: float(x))\n",
    "#x_train['sqft_basement'].dtype\n",
    "x_test1['sqft_basement'] = x_test1['sqft_basement'].apply(lambda x: float(x.replace(\"?\", \"297\")))\n",
    "x_test1['sqft_basement'] = x_test1['sqft_basement'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58faca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1['waterfront'] = x_train1['waterfront'].fillna(value = 'NO')\n",
    "x_train1['waterfront'] = x_train1['waterfront'].replace(to_replace = ['YES','NO'],value = [1,0])\n",
    "#x_train1['waterfront'].value_counts()\n",
    "x_test1['waterfront'] = x_test1['waterfront'].fillna(value = 'NO')\n",
    "x_test1['waterfront'] = x_test1['waterfront'].replace(to_replace = ['YES','NO'],value = [1,0])\n",
    "#x_test1['waterfront'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5fb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na with no value -->majority is none and could be N/A as none as well. n change the catgorical ordinal in to 0 to 4 with 0 to none and 4 to excellent\n",
    "x_train1['view'] = x_train1['view'].fillna(value = 'NONE')\n",
    "x_test1['view'] = x_test1['view'].fillna(value = 'NONE')\n",
    "x_train1['view'] = x_train1['view'].replace(to_replace = ['NONE','AVERAGE','GOOD','FAIR','EXCELLENT'],value = [0,1,2,3,4])\n",
    "x_test1['view'] = x_test1['view'].replace(to_replace = ['NONE','AVERAGE','GOOD','FAIR','EXCELLENT'],value = [0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c521ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace cateogrical rating with (0 to 5 scale. 0 - Poor, and 4 - Very Good)\n",
    "x_train1['condition'] = x_train1['condition'].replace(to_replace = ['Poor','Fair','Average','Good','Very Good'],value = [0,1,2,3,4])\n",
    "x_test1['condition'] = x_test1['condition'].replace(to_replace = ['Poor','Fair','Average','Good','Very Good'],value = [0,1,2,3,4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc20ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the numerical rating and assign it to interger type\n",
    "x_train1['new_grade'] = x_train1['grade'].astype(str).str[0]\n",
    "x_train1['new_grade'] = x_train1['new_grade'].astype(int)\n",
    "x_test1['new_grade'] = x_test1['grade'].astype(str).str[0]\n",
    "x_test1['new_grade'] = x_test1['new_grade'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c3b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the string grade column\n",
    "x_train1 = x_train1.drop(columns='grade')\n",
    "x_test1 = x_test1.drop(columns='grade')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping seattle into zipcode which it solely belongs https://www.ciclt.net/sn/clt/capitolimpact/gw_ziplist.aspx?FIPS=53033 \n",
    "x_train1['seattle'] = x_train1['zipcode'].apply(lambda x : 1 if x == 98101 or x == 98102 or x == 98103 or x == 98104 or x == 98105 or x == 98106 or x == 98107 or x == 98109 or x == 98111 or x == 98112 or x == 98114 or x == 98116 or x == 98117 or x == 98118 or x == 98119 or x == 98121 or x == 98122 or x == 98124 or x == 98125 or x == 98126 or x == 98131 or x == 98132 or x == 98133 or x == 98134 or x == 98136 or x == 98144 or x == 98145 or x == 98146 or x == 98148 or x == 98154  or x == 98160 or x == 98161 or x == 98164 or x == 98166 or x == 98171 or x == 98174 or x== 98178 or x == 98199 else 0) \n",
    "x_test1['seattle'] = x_test1['zipcode'].apply(lambda x : 1 if x == 98101 or x == 98102 or x == 98103 or x == 98104 or x == 98105 or x == 98106 or x == 98107 or x == 98109 or x == 98111 or x == 98112 or x == 98114 or x == 98116 or x == 98117 or x == 98118 or x == 98119 or x == 98121 or x == 98122 or x == 98124 or x == 98125 or x == 98126 or x == 98131 or x == 98132 or x == 98133 or x == 98134 or x == 98136 or x == 98144 or x == 98145 or x == 98146 or x == 98148 or x == 98154  or x == 98160 or x == 98161 or x == 98164 or x == 98166 or x == 98171 or x == 98174 or x== 98178 or x == 98199 else 0) \n",
    "x_train1['kent'] = x_train1['zipcode'].apply(lambda x : 1 if x == 98030 or x == 98031 or x == 98032 or x == 98035 or x == 98042 or x == 98064 else 0)\n",
    "x_test1['kent'] = x_test1['zipcode'].apply(lambda x : 1 if x == 98030 or x == 98031 or x == 98032 or x == 98035 or x == 98042 or x == 98064 else 0)\n",
    "x_train1['bellevue'] = x_train1['zipcode'].apply(lambda x : 1 if x == 98004 or x == 98005 or x == 98006 or x == 98007 or x == 98008 or x == 98009 or x == 98015 else 0)\n",
    "x_test1['bellevue'] = x_test1['zipcode'].apply(lambda x : 1 if x == 98004 or x == 98005 or x == 98006 or x == 98007 or x == 98008 or x == 98009 or x == 98015 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbacd08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71552012",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1['seattle'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14885f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64d6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot Econdinf zipcode for x1_train\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "data =  x_train1[['zipcode']]\n",
    "\n",
    "view_df = pd.DataFrame(ohe.fit_transform(data).toarray(), index = x_train1.index) #index = x_train.index to match x_train index for concat later\n",
    "view_df.columns = ohe.get_feature_names()   #use get_feature_names() to get feature name back after one hot encoding\n",
    "\n",
    "x_train1 = pd.concat([x_train1,view_df],axis=1)\n",
    "x_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot Econdinf zipcode for x_test\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "data =  x_test1[['zipcode']]\n",
    "\n",
    "view_df = pd.DataFrame(ohe.fit_transform(data).toarray(), index = x_test1.index) #index = x_train.index to match x_train index for concat later\n",
    "view_df.columns = ohe.get_feature_names()   #use get_feature_names() to get feature name back after one hot encoding\n",
    "\n",
    "x_test1 = pd.concat([x_test1,view_df],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c40e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna with 0 to NaN for year_renovated- assuming there is no renovation.\n",
    "x_train1['yr_renovated'] = x_train1['yr_renovated'].fillna(value = 0)\n",
    "x_test1['yr_renovated'] = x_test1['yr_renovated'].fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c16920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a log on price\n",
    "y_train1 = np.log(y_train1)\n",
    "y_test1 = np.log(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data w MinMaxScaler 'sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15 for both train n data set\n",
    "features = ['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15']\n",
    "autoscaler = MinMaxScaler()\n",
    "\n",
    "x_train1[features] = autoscaler.fit_transform(x_train1[features])\n",
    "x_test1[features] = autoscaler.fit_transform(x_test1[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4760d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train1 = x_train1[['bedrooms', 'bathrooms', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15', 'month', 'new_grade', 'x0_98002', 'x0_98003', 'x0_98005', 'x0_98007', 'x0_98010', 'x0_98022', 'x0_98023', 'x0_98024', 'x0_98030', 'x0_98031', 'x0_98032', 'x0_98040', 'x0_98042', 'x0_98055', 'x0_98056', 'x0_98058', 'x0_98070', 'x0_98092', 'x0_98108', 'x0_98168', 'x0_98188', 'x0_98198','seattle','kent','bellevue']]\n",
    "x_test1 = x_test1[['bedrooms', 'bathrooms', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15', 'month', 'new_grade', 'x0_98002', 'x0_98003', 'x0_98005', 'x0_98007', 'x0_98010', 'x0_98022', 'x0_98023', 'x0_98024', 'x0_98030', 'x0_98031', 'x0_98032', 'x0_98040', 'x0_98042', 'x0_98055', 'x0_98056', 'x0_98058', 'x0_98070', 'x0_98092', 'x0_98108', 'x0_98168', 'x0_98188', 'x0_98198','seattle','kent','bellevue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "y = y_train1\n",
    "X= x_train1\n",
    "z = x_test1\n",
    "Hh = y_test1\n",
    "Xcont = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(endog = y, exog = Xcont)\n",
    "model = sm.OLS(y,Xcont)\n",
    "res = model.fit()\n",
    "#testmodel = sm.OLS(z,Hh).fit()\n",
    "#print(testmodel.summary())\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495af7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols3 = LinearRegression()\n",
    "testsmodel = ols3.fit(X,y)\n",
    "print(testsmodel.score(X,y)) # train\n",
    "print(testsmodel.score(z,Hh)) # test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6955105",
   "metadata": {},
   "source": [
    "Model3 R2 is at 0.65. However, and some outliers is eliminated while R2 score decreases. \n",
    "\n",
    "The Training and Test scores are with 2%, so it shows me that the our model is not underfit/overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08940a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.exp(model3_predictions), np.exp(y_test1))\n",
    "plt.xlabel('predicting testing price in $')\n",
    "plt.ylabel('actual testing sale price in $')\n",
    "fig.suptitle('prediction price vs actual sale price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b13be7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot((y_test1-model3_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0cc7e",
   "metadata": {},
   "source": [
    "QQ plot: points forming a line thatâ€™s roughly straight line- left tail is reducing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea06fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(res.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef90eb56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.exp(model3_predictions),(np.exp(y_test1)-np.exp(model3_predictions)))\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.title(\"model3 residual plot, testing set residual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded01f3",
   "metadata": {},
   "source": [
    "# Prediction with model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1414731c",
   "metadata": {},
   "source": [
    "While the R2 score went down to 63%, I decide to go with model with fewer outliers and less predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8cb5a2",
   "metadata": {},
   "source": [
    "here is the prediction w the training set and the residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction w the x_train set\n",
    "model3_train_prediction = ols3.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d8c667",
   "metadata": {},
   "source": [
    "make a plot between x_train prediction vs y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff1207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.exp(model3_train_prediction), np.exp(y_train1))\n",
    "plt.xlabel('predicting testing price in $')\n",
    "plt.ylabel('actual testing sale price in $')\n",
    "fig.suptitle('prediction price vs actual sale price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57c469",
   "metadata": {},
   "source": [
    "residual plot of model_3 train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d618baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.exp(model3_train_prediction),(np.exp(y_train1)-np.exp(model3_train_prediction)))\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.title(\"model3 residual plot, train set residual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb71139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the intercept\n",
    "ols3.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fint the coefficient\n",
    "ols3.coefficient_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5252ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find predicted value with test set\n",
    "model3_predictions = ols3.predict(z)\n",
    "model3_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6233678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute a dataframe with city and predict and actual price.\n",
    "df_predict_price = pd.DataFrame(model3_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_price = pd.DataFrame({'seattle':z['seattle'],'kent':z['kent'],'bellevue':z['bellevue'],'actual':np.exp(y_test1),'predicted':np.exp(model3_predictions)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740b404",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_seattle = df_predict_price.loc[df_predict_price['seattle'] ==1]\n",
    "df_seattle.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0cb04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kent = df_predict_price.loc[df_predict_price['kent'] ==1]\n",
    "df_kent.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bellevue = df_predict_price.loc[df_predict_price['bellevue'] ==1]\n",
    "df_bellevue.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719bb582",
   "metadata": {},
   "source": [
    "# Recommendations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b9234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20c2f67a",
   "metadata": {},
   "source": [
    "# Next Steps:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f67ef",
   "metadata": {},
   "source": [
    "While this analysis can help us analyse house sale, there are some additional steps we can take to provide a more detailed analysis.\n",
    "\n",
    "Investigate with different typr of models such as polynomial models that may fit predictors that may not have a linear relationship with the outcome instead of linear model\n",
    "\n",
    "Continue to collect data for house sale during covid. Are we starting to see the house sale depends on sqft_living, bedroom and bathroom size since majority of the people are working from home.\n",
    "\n",
    "Adjust house sale for inflation. We may get a more accurate analysis if house sale is adjusted for inflation. \n",
    "\n",
    "Collect additional data for the upcoming year and more. As recession is looming, and the Fed decides to increrase the interest rate, would we able to analyze it and how it impacts the house sale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
